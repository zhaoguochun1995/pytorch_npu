// Copyright (c) 2023 Huawei Technologies Co., Ltd
// Copyright (c) 2022, Facebook CORPORATION.
// All rights reserved.
//
// Licensed under the BSD 3-Clause License  (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// https://opensource.org/licenses/BSD-3-Clause
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include <torch/csrc/autograd/VariableTypeUtils.h>

#include <ATen/RedispatchFunctions.h>
#include <torch/library.h>

#include "torch_npu/csrc/aten/NPUNativeFunctions.h"
#include "torch_npu/csrc/aten/VariableType.h"
#include "torch_npu/csrc/framework/autograd/FunctionsManual.h"
#include "op_plugin/OpInterface.h"

// @generated by tools/codegen/gen.py from VariableTypeNPU.cpp

// NOTE [Sharded File]: on this file's split-into-shards state
//
// Back in the good old days, VariableType.cpp was generated as one
// file with every function in it, and everything was great and
// simple.
//
// However, this file was also very large (over 36,000 lines), and
// compiling it was very slow, and in fact was a significant
// bottleneck for incremental rebuilds. To address this, we now
// generate the file split across multiple shards, named
// VariableType_0.cpp and so on, which can be compiled in parallel.
//
// For ease of inspection and debugging, so that it's not necessary to
// go rooting around in multiple files, we also generate all the
// functions together in VariableTypeEverything.cpp. This generated
// file is only for convenience; it's not actually used in the
// build. If the file you're looking at now is one of the shards, you
// may want to switch over to the Everything variant to make you
// grepping smoother.

using namespace at;
using namespace at_npu::autograd::generated;
using namespace at_npu::autograd::generated::details;

namespace at_npu { namespace autograd {

namespace VariableType {
namespace {
  C10_UNUSED void reset_grad_accumulator(Variable & self) {
    AutogradMeta* meta = torch::autograd::impl::get_autograd_meta(self);
    if (meta != nullptr) {
      meta->grad_accumulator_.reset();
    }
  }
}

at::Tensor _npu_format_cast(c10::DispatchKeySet ks, const at::Tensor & self, int64_t acl_format) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuFormatCastBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuFormatCastBackward0>(new NpuFormatCastBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at_npu::native::NPUNativeFunctions::_npu_format_cast(self_, acl_format);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: _npu_format_cast");
  AT_ASSERT(result.use_count() <= 1, "function: _npu_format_cast");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "_npu_format_cast");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with _npu_format_cast that does not support it.");
  return result;
}
at::Tensor fast_gelu(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<FastGeluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<FastGeluBackward0>(new FastGeluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::fast_gelu(self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: fast_gelu");
  AT_ASSERT(result.use_count() <= 1, "function: fast_gelu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "fast_gelu");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with fast_gelu that does not support it.");
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> npu_fused_attention_score_fwd(c10::DispatchKeySet ks, const at::Tensor & query_layer, const at::Tensor & key_layer, const at::Tensor & value_layer, const at::Tensor & attention_mask, const at::Scalar & scale, double keep_prob, bool query_transpose, bool key_transpose, bool bmm_score_transpose_a, bool bmm_score_transpose_b, bool value_transpose, bool dx_transpose) {
  auto& query_layer_ = unpack(query_layer, "query_layer", 0);
  auto& key_layer_ = unpack(key_layer, "key_layer", 1);
  auto& value_layer_ = unpack(value_layer, "value_layer", 2);
  auto& attention_mask_ = unpack(attention_mask, "attention_mask", 3);
  auto _any_requires_grad = compute_requires_grad( query_layer, key_layer, value_layer );
  
  (void)_any_requires_grad;
  check_no_requires_grad(attention_mask, "attention_mask", "npu_fused_attention_score_fwd");
  std::shared_ptr<NpuFusedAttentionScoreFwdBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuFusedAttentionScoreFwdBackward0>(new NpuFusedAttentionScoreFwdBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( query_layer, key_layer, value_layer ));
    grad_fn->query_layer_ = SavedVariable(query_layer, false);
    grad_fn->key_layer_ = SavedVariable(key_layer, false);
    grad_fn->value_layer_ = SavedVariable(value_layer, false);
    grad_fn->scale = scale;
    grad_fn->keep_prob = keep_prob;
    grad_fn->query_transpose = query_transpose;
    grad_fn->key_transpose = key_transpose;
    grad_fn->value_transpose = value_transpose;
    grad_fn->dx_transpose = dx_transpose;
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> query_layer__storage_saved =
    query_layer_.has_storage() ? c10::optional<Storage>(query_layer_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query_layer__impl_saved;
  if (query_layer_.defined()) query_layer__impl_saved = query_layer_.getIntrusivePtr();
  c10::optional<Storage> key_layer__storage_saved =
    key_layer_.has_storage() ? c10::optional<Storage>(key_layer_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key_layer__impl_saved;
  if (key_layer_.defined()) key_layer__impl_saved = key_layer_.getIntrusivePtr();
  c10::optional<Storage> value_layer__storage_saved =
    value_layer_.has_storage() ? c10::optional<Storage>(value_layer_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value_layer__impl_saved;
  if (value_layer_.defined()) value_layer__impl_saved = value_layer_.getIntrusivePtr();
  c10::optional<Storage> attention_mask__storage_saved =
    attention_mask_.has_storage() ? c10::optional<Storage>(attention_mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> attention_mask__impl_saved;
  if (attention_mask_.defined()) attention_mask__impl_saved = attention_mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_fused_attention_score_fwd(query_layer_, key_layer_, value_layer_, attention_mask_, scale, keep_prob, query_transpose, key_transpose, bmm_score_transpose_a, bmm_score_transpose_b, value_transpose, dx_transpose);
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (query_layer__storage_saved.has_value())
    AT_ASSERT(query_layer__storage_saved.value().is_alias_of(query_layer_.storage()));
  if (query_layer__impl_saved) AT_ASSERT(query_layer__impl_saved == query_layer_.getIntrusivePtr());
  if (key_layer__storage_saved.has_value())
    AT_ASSERT(key_layer__storage_saved.value().is_alias_of(key_layer_.storage()));
  if (key_layer__impl_saved) AT_ASSERT(key_layer__impl_saved == key_layer_.getIntrusivePtr());
  if (value_layer__storage_saved.has_value())
    AT_ASSERT(value_layer__storage_saved.value().is_alias_of(value_layer_.storage()));
  if (value_layer__impl_saved) AT_ASSERT(value_layer__impl_saved == value_layer_.getIntrusivePtr());
  if (attention_mask__storage_saved.has_value())
    AT_ASSERT(attention_mask__storage_saved.value().is_alias_of(attention_mask_.storage()));
  if (attention_mask__impl_saved) AT_ASSERT(attention_mask__impl_saved == attention_mask_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_fused_attention_score_fwd");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_fused_attention_score_fwd");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_fused_attention_score_fwd");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_fused_attention_score_fwd");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_fused_attention_score_fwd");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_fused_attention_score_fwd");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_fused_attention_score_fwd");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(query_layer) || isFwGradDefined(key_layer) || isFwGradDefined(value_layer) || isFwGradDefined(attention_mask)), "Trying to use forward AD with npu_fused_attention_score_fwd that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor npu_rotary_mul(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & r1, const at::Tensor & r2) {
  auto& self_ = unpack(self, "self", 0);
  auto& r1_ = unpack(r1, "r1", 1);
  auto& r2_ = unpack(r2, "r2", 2);
  auto _any_requires_grad = compute_requires_grad( self, r1, r2 );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuRotaryMulBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuRotaryMulBackward0>(new NpuRotaryMulBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, r1, r2 ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->r1_ = SavedVariable(r1, false);
    grad_fn->r2_ = SavedVariable(r2, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> r1__storage_saved =
    r1_.has_storage() ? c10::optional<Storage>(r1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> r1__impl_saved;
  if (r1_.defined()) r1__impl_saved = r1_.getIntrusivePtr();
  c10::optional<Storage> r2__storage_saved =
    r2_.has_storage() ? c10::optional<Storage>(r2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> r2__impl_saved;
  if (r2_.defined()) r2__impl_saved = r2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_rotary_mul(self_, r1_, r2_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (r1__storage_saved.has_value())
    AT_ASSERT(r1__storage_saved.value().is_alias_of(r1_.storage()));
  if (r1__impl_saved) AT_ASSERT(r1__impl_saved == r1_.getIntrusivePtr());
  if (r2__storage_saved.has_value())
    AT_ASSERT(r2__storage_saved.value().is_alias_of(r2_.storage()));
  if (r2__impl_saved) AT_ASSERT(r2__impl_saved == r2_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_rotary_mul");
  AT_ASSERT(result.use_count() <= 1, "function: npu_rotary_mul");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_rotary_mul");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(r1) || isFwGradDefined(r2)), "Trying to use forward AD with npu_rotary_mul that does not support it.");
  return result;
}
at::Tensor npu_convolution(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuConvolutionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuConvolutionBackward0>(new NpuConvolutionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
    grad_fn->stride = stride.vec();
    grad_fn->padding = padding.vec();
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_convolution(input_, weight_, bias, stride, padding, dilation, groups);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value())
    AT_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved) AT_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_convolution");
  AT_ASSERT(result.use_count() <= 1, "function: npu_convolution");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_convolution");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias)), "Trying to use forward AD with npu_convolution that does not support it.");
  return result;
}
at::Tensor npu_convolution_transpose(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuConvolutionTransposeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuConvolutionTransposeBackward0>(new NpuConvolutionTransposeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
    grad_fn->padding = padding.vec();
    grad_fn->output_padding = output_padding.vec();
    grad_fn->stride = stride.vec();
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_convolution_transpose(input_, weight_, bias, padding, output_padding, stride, dilation, groups);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value())
    AT_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved) AT_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_convolution_transpose");
  AT_ASSERT(result.use_count() <= 1, "function: npu_convolution_transpose");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_convolution_transpose");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias)), "Trying to use forward AD with npu_convolution_transpose that does not support it.");
  return result;
}
at::Tensor npu_confusion_transpose(c10::DispatchKeySet ks, const at::Tensor & self, at::IntArrayRef perm, at::IntArrayRef shape, bool transpose_first) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuConfusionTransposeBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuConfusionTransposeBackward0>(new NpuConfusionTransposeBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->perm = perm.vec();
    grad_fn->transpose_first = transpose_first;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_confusion_transpose(self_, perm, shape, transpose_first);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_confusion_transpose");
  AT_ASSERT(result.use_count() <= 1, "function: npu_confusion_transpose");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_confusion_transpose");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_confusion_transpose that does not support it.");
  return result;
}
at::Tensor npu_ps_roi_pooling(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & rois, double spatial_scale, int64_t group_size, int64_t output_dim) {
  auto& self_ = unpack(self, "self", 0);
  auto& rois_ = unpack(rois, "rois", 1);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  check_no_requires_grad(rois, "rois", "npu_ps_roi_pooling");
  std::shared_ptr<NpuPsRoiPoolingBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuPsRoiPoolingBackward0>(new NpuPsRoiPoolingBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->rois_ = SavedVariable(rois, false);
    grad_fn->spatial_scale = spatial_scale;
    grad_fn->group_size = group_size;
    grad_fn->output_dim = output_dim;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> rois__storage_saved =
    rois_.has_storage() ? c10::optional<Storage>(rois_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> rois__impl_saved;
  if (rois_.defined()) rois__impl_saved = rois_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_ps_roi_pooling(self_, rois_, spatial_scale, group_size, output_dim);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (rois__storage_saved.has_value())
    AT_ASSERT(rois__storage_saved.value().is_alias_of(rois_.storage()));
  if (rois__impl_saved) AT_ASSERT(rois__impl_saved == rois_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_ps_roi_pooling");
  AT_ASSERT(result.use_count() <= 1, "function: npu_ps_roi_pooling");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_ps_roi_pooling");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(rois)), "Trying to use forward AD with npu_ps_roi_pooling that does not support it.");
  return result;
}
at::Tensor npu_linear(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto _any_requires_grad = compute_requires_grad( input, weight, bias );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuLinearBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuLinearBackward0>(new NpuLinearBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_linear(input_, weight_, bias);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value())
    AT_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved) AT_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_linear");
  AT_ASSERT(result.use_count() <= 1, "function: npu_linear");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_linear");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias)), "Trying to use forward AD with npu_linear that does not support it.");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> _npu_dropout(c10::DispatchKeySet ks, const at::Tensor & self, double p) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuDropoutBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuDropoutBackward0>(new NpuDropoutBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->p = p;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::_npu_dropout(self_, p);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: _npu_dropout");
  AT_ASSERT(result0.use_count() <= 1, "function: _npu_dropout");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: _npu_dropout");
  AT_ASSERT(result1.use_count() <= 1, "function: _npu_dropout");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_npu_dropout");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with _npu_dropout that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor npu_softmax_cross_entropy_with_logits(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & labels) {
  auto& self_ = unpack(self, "self", 0);
  auto& labels_ = unpack(labels, "labels", 1);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  check_no_requires_grad(labels, "labels", "npu_softmax_cross_entropy_with_logits");
  std::shared_ptr<NpuSoftmaxCrossEntropyWithLogitsBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuSoftmaxCrossEntropyWithLogitsBackward0>(new NpuSoftmaxCrossEntropyWithLogitsBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->labels_ = SavedVariable(labels, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> labels__storage_saved =
    labels_.has_storage() ? c10::optional<Storage>(labels_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> labels__impl_saved;
  if (labels_.defined()) labels__impl_saved = labels_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_softmax_cross_entropy_with_logits(self_, labels_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (labels__storage_saved.has_value())
    AT_ASSERT(labels__storage_saved.value().is_alias_of(labels_.storage()));
  if (labels__impl_saved) AT_ASSERT(labels__impl_saved == labels_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_softmax_cross_entropy_with_logits");
  AT_ASSERT(result.use_count() <= 1, "function: npu_softmax_cross_entropy_with_logits");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_softmax_cross_entropy_with_logits");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(labels)), "Trying to use forward AD with npu_softmax_cross_entropy_with_logits that does not support it.");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> npu_max_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuMaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuMaxBackward0>(new NpuMaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_max(self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage()) AT_ASSERT(values.storage().use_count() == 1, "function: npu_max_dim");
  AT_ASSERT(values.use_count() <= 1, "function: npu_max_dim");
  if (indices.has_storage()) AT_ASSERT(indices.storage().use_count() == 1, "function: npu_max_dim");
  AT_ASSERT(indices.use_count() <= 1, "function: npu_max_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "npu_max");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_max that does not support it.");
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
at::Tensor npu_bmmV2(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mat2, at::IntArrayRef output_sizes) {
  auto& self_ = unpack(self, "self", 0);
  auto& mat2_ = unpack(mat2, "mat2", 1);
  auto _any_requires_grad = compute_requires_grad( self, mat2 );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuBmmv2Backward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuBmmv2Backward0>(new NpuBmmv2Backward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, mat2 ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->mat2_ = SavedVariable(mat2, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mat2__storage_saved =
    mat2_.has_storage() ? c10::optional<Storage>(mat2_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mat2__impl_saved;
  if (mat2_.defined()) mat2__impl_saved = mat2_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_bmmV2(self_, mat2_, output_sizes);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mat2__storage_saved.has_value())
    AT_ASSERT(mat2__storage_saved.value().is_alias_of(mat2_.storage()));
  if (mat2__impl_saved) AT_ASSERT(mat2__impl_saved == mat2_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_bmmV2");
  AT_ASSERT(result.use_count() <= 1, "function: npu_bmmV2");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_bmmV2");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(mat2)), "Trying to use forward AD with npu_bmmV2 that does not support it.");
  return result;
}
at::Tensor npu_dtype_cast(c10::DispatchKeySet ks, const at::Tensor & self, at::ScalarType dtype) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuDtypeCastBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuDtypeCastBackward0>(new NpuDtypeCastBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_scalar_type = self.scalar_type();
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_dtype_cast(self_, dtype);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_dtype_cast");
  AT_ASSERT(result.use_count() <= 1, "function: npu_dtype_cast");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_dtype_cast");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_dtype_cast that does not support it.");
  return result;
}
at::Tensor npu_silu(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuSiluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuSiluBackward0>(new NpuSiluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_silu(self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_silu");
  AT_ASSERT(result.use_count() <= 1, "function: npu_silu");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_silu");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_silu that does not support it.");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
at::Tensor & npu_silu_(c10::DispatchKeySet ks, at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  check_inplace(self, _any_requires_grad);
  c10::optional<at::Tensor> original_self;
  std::shared_ptr<NpuSiluBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuSiluBackward0>(new NpuSiluBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    if (!original_self.has_value()) original_self = self.clone();
    grad_fn->self_ = SavedVariable(original_self.value(), false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    op_plugin::npu_silu_(self_);
  }
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  #endif
  if (grad_fn) {
      rebase_history(flatten_tensor_args( self ), grad_fn);
  }
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_silu_ that does not support it.");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(self, true, self.is_view());
  }
  return self;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> npu_gru(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & weight_input, const at::Tensor & weight_hidden, const at::Tensor & bias_input, const at::Tensor & bias_hidden, const at::Tensor & seq_length, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
  auto& input_ = unpack(input, "input", 0);
  auto& hx_ = unpack(hx, "hx", 1);
  auto& weight_input_ = unpack(weight_input, "weight_input", 2);
  auto& weight_hidden_ = unpack(weight_hidden, "weight_hidden", 3);
  auto& bias_input_ = unpack(bias_input, "bias_input", 4);
  auto& bias_hidden_ = unpack(bias_hidden, "bias_hidden", 5);
  auto& seq_length_ = unpack(seq_length, "seq_length", 6);
  auto _any_requires_grad = compute_requires_grad( input, hx, weight_input, weight_hidden, bias_input, bias_hidden );
  
  (void)_any_requires_grad;
  check_no_requires_grad(seq_length, "seq_length", "npu_gru");
  std::shared_ptr<NpuGruBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuGruBackward0>(new NpuGruBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, hx, weight_input, weight_hidden, bias_input, bias_hidden ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->hx_ = SavedVariable(hx, false);
    grad_fn->weight_input_ = SavedVariable(weight_input, false);
    grad_fn->weight_hidden_ = SavedVariable(weight_hidden, false);
    grad_fn->bias_input_ = SavedVariable(bias_input, false);
    grad_fn->bias_hidden_ = SavedVariable(bias_hidden, false);
    grad_fn->seq_length_ = SavedVariable(seq_length, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  at::Tensor result4;
  at::Tensor result5;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> hx__storage_saved =
    hx_.has_storage() ? c10::optional<Storage>(hx_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> hx__impl_saved;
  if (hx_.defined()) hx__impl_saved = hx_.getIntrusivePtr();
  c10::optional<Storage> weight_input__storage_saved =
    weight_input_.has_storage() ? c10::optional<Storage>(weight_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight_input__impl_saved;
  if (weight_input_.defined()) weight_input__impl_saved = weight_input_.getIntrusivePtr();
  c10::optional<Storage> weight_hidden__storage_saved =
    weight_hidden_.has_storage() ? c10::optional<Storage>(weight_hidden_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight_hidden__impl_saved;
  if (weight_hidden_.defined()) weight_hidden__impl_saved = weight_hidden_.getIntrusivePtr();
  c10::optional<Storage> bias_input__storage_saved =
    bias_input_.has_storage() ? c10::optional<Storage>(bias_input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> bias_input__impl_saved;
  if (bias_input_.defined()) bias_input__impl_saved = bias_input_.getIntrusivePtr();
  c10::optional<Storage> bias_hidden__storage_saved =
    bias_hidden_.has_storage() ? c10::optional<Storage>(bias_hidden_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> bias_hidden__impl_saved;
  if (bias_hidden_.defined()) bias_hidden__impl_saved = bias_hidden_.getIntrusivePtr();
  c10::optional<Storage> seq_length__storage_saved =
    seq_length_.has_storage() ? c10::optional<Storage>(seq_length_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> seq_length__impl_saved;
  if (seq_length_.defined()) seq_length__impl_saved = seq_length_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_gru(input_, hx_, weight_input_, weight_hidden_, bias_input_, bias_hidden_, seq_length_, has_biases, num_layers, dropout, train, bidirectional, batch_first);
  })();
  std::tie(result0, result1, result2, result3, result4, result5) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (hx__storage_saved.has_value())
    AT_ASSERT(hx__storage_saved.value().is_alias_of(hx_.storage()));
  if (hx__impl_saved) AT_ASSERT(hx__impl_saved == hx_.getIntrusivePtr());
  if (weight_input__storage_saved.has_value())
    AT_ASSERT(weight_input__storage_saved.value().is_alias_of(weight_input_.storage()));
  if (weight_input__impl_saved) AT_ASSERT(weight_input__impl_saved == weight_input_.getIntrusivePtr());
  if (weight_hidden__storage_saved.has_value())
    AT_ASSERT(weight_hidden__storage_saved.value().is_alias_of(weight_hidden_.storage()));
  if (weight_hidden__impl_saved) AT_ASSERT(weight_hidden__impl_saved == weight_hidden_.getIntrusivePtr());
  if (bias_input__storage_saved.has_value())
    AT_ASSERT(bias_input__storage_saved.value().is_alias_of(bias_input_.storage()));
  if (bias_input__impl_saved) AT_ASSERT(bias_input__impl_saved == bias_input_.getIntrusivePtr());
  if (bias_hidden__storage_saved.has_value())
    AT_ASSERT(bias_hidden__storage_saved.value().is_alias_of(bias_hidden_.storage()));
  if (bias_hidden__impl_saved) AT_ASSERT(bias_hidden__impl_saved == bias_hidden_.getIntrusivePtr());
  if (seq_length__storage_saved.has_value())
    AT_ASSERT(seq_length__storage_saved.value().is_alias_of(seq_length_.storage()));
  if (seq_length__impl_saved) AT_ASSERT(seq_length__impl_saved == seq_length_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_gru");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_gru");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_gru");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_gru");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_gru");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_gru");
  if (result3.has_storage()) AT_ASSERT(result3.storage().use_count() == 1, "function: npu_gru");
  AT_ASSERT(result3.use_count() <= 1, "function: npu_gru");
  if (result4.has_storage()) AT_ASSERT(result4.storage().use_count() == 1, "function: npu_gru");
  AT_ASSERT(result4.use_count() <= 1, "function: npu_gru");
  if (result5.has_storage()) AT_ASSERT(result5.storage().use_count() == 1, "function: npu_gru");
  AT_ASSERT(result5.use_count() <= 1, "function: npu_gru");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_gru");
  throw_error_for_complex_autograd(result1, "npu_gru");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(hx) || isFwGradDefined(weight_input) || isFwGradDefined(weight_hidden) || isFwGradDefined(bias_input) || isFwGradDefined(bias_hidden) || isFwGradDefined(seq_length)), "Trying to use forward AD with npu_gru that does not support it.");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4_ = SavedVariable(result4, true);
    grad_fn->result5_ = SavedVariable(result5, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4), std::move(result5));
}
at::Tensor npu_mish(c10::DispatchKeySet ks, const at::Tensor & self) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuMishBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuMishBackward0>(new NpuMishBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_mish(self_);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_mish");
  AT_ASSERT(result.use_count() <= 1, "function: npu_mish");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_mish");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_mish that does not support it.");
  return result;
}
::std::tuple<at::Tensor,at::Tensor> npu_min_dim(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool keepdim) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuMinBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuMinBackward0>(new NpuMinBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->dim = dim;
    grad_fn->keepdim = keepdim;
  }
  at::Tensor values;
  at::Tensor indices;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_min(self_, dim, keepdim);
  })();
  std::tie(values, indices) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (values.has_storage()) AT_ASSERT(values.storage().use_count() == 1, "function: npu_min_dim");
  AT_ASSERT(values.use_count() <= 1, "function: npu_min_dim");
  if (indices.has_storage()) AT_ASSERT(indices.storage().use_count() == 1, "function: npu_min_dim");
  AT_ASSERT(indices.use_count() <= 1, "function: npu_min_dim");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( values ), grad_fn);
  }
  throw_error_for_complex_autograd(values, "npu_min");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with npu_min that does not support it.");
  if (grad_fn) {
    grad_fn->indices_ = SavedVariable(indices, true);
  }
  return std::make_tuple(std::move(values), std::move(indices));
}
::std::tuple<at::Tensor,at::Tensor> npu_deformable_conv2d(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & offset, const c10::optional<at::Tensor> & bias, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups, int64_t deformable_groups, bool modulated) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto& offset_ = unpack(offset, "offset", 2);
  auto _any_requires_grad = compute_requires_grad( input, weight, offset, bias );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuDeformableConv2DBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuDeformableConv2DBackward0>(new NpuDeformableConv2DBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, offset, bias ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
    grad_fn->offset_ = SavedVariable(offset, false);
    grad_fn->kernel_size = kernel_size.vec();
    grad_fn->stride = stride.vec();
    grad_fn->padding = padding.vec();
    grad_fn->dilation = dilation.vec();
    grad_fn->groups = groups;
    grad_fn->deformable_groups = deformable_groups;
    grad_fn->modulated = modulated;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> offset__storage_saved =
    offset_.has_storage() ? c10::optional<Storage>(offset_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> offset__impl_saved;
  if (offset_.defined()) offset__impl_saved = offset_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_deformable_conv2d(input_, weight_, offset_, bias, kernel_size, stride, padding, dilation, groups, deformable_groups, modulated);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value())
    AT_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved) AT_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (offset__storage_saved.has_value())
    AT_ASSERT(offset__storage_saved.value().is_alias_of(offset_.storage()));
  if (offset__impl_saved) AT_ASSERT(offset__impl_saved == offset_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_deformable_conv2d");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_deformable_conv2d");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_deformable_conv2d");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_deformable_conv2d");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_deformable_conv2d");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(offset) || isFwGradDefined(bias)), "Trying to use forward AD with npu_deformable_conv2d that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
at::Tensor npu_giou(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & gtboxes, bool trans, bool is_cross, int64_t mode) {
  auto& self_ = unpack(self, "self", 0);
  auto& gtboxes_ = unpack(gtboxes, "gtboxes", 1);
  auto _any_requires_grad = compute_requires_grad( self, gtboxes );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuGiouBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuGiouBackward0>(new NpuGiouBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, gtboxes ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->gtboxes_ = SavedVariable(gtboxes, false);
    grad_fn->trans = trans;
    grad_fn->is_cross = is_cross;
    grad_fn->mode = mode;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> gtboxes__storage_saved =
    gtboxes_.has_storage() ? c10::optional<Storage>(gtboxes_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> gtboxes__impl_saved;
  if (gtboxes_.defined()) gtboxes__impl_saved = gtboxes_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_giou(self_, gtboxes_, trans, is_cross, mode);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (gtboxes__storage_saved.has_value())
    AT_ASSERT(gtboxes__storage_saved.value().is_alias_of(gtboxes_.storage()));
  if (gtboxes__impl_saved) AT_ASSERT(gtboxes__impl_saved == gtboxes_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_giou");
  AT_ASSERT(result.use_count() <= 1, "function: npu_giou");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_giou");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(gtboxes)), "Trying to use forward AD with npu_giou that does not support it.");
  return result;
}
at::Tensor npu_diou(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & gtboxes, bool trans, bool is_cross, int64_t mode) {
  auto& self_ = unpack(self, "self", 0);
  auto& gtboxes_ = unpack(gtboxes, "gtboxes", 1);
  auto _any_requires_grad = compute_requires_grad( self, gtboxes );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuDiouBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuDiouBackward0>(new NpuDiouBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, gtboxes ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->gtboxes_ = SavedVariable(gtboxes, false);
    grad_fn->trans = trans;
    grad_fn->is_cross = is_cross;
    grad_fn->mode = mode;
  }
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> gtboxes__storage_saved =
    gtboxes_.has_storage() ? c10::optional<Storage>(gtboxes_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> gtboxes__impl_saved;
  if (gtboxes_.defined()) gtboxes__impl_saved = gtboxes_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_diou(self_, gtboxes_, trans, is_cross, mode);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (gtboxes__storage_saved.has_value())
    AT_ASSERT(gtboxes__storage_saved.value().is_alias_of(gtboxes_.storage()));
  if (gtboxes__impl_saved) AT_ASSERT(gtboxes__impl_saved == gtboxes_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_diou");
  AT_ASSERT(result.use_count() <= 1, "function: npu_diou");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_diou");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(gtboxes)), "Trying to use forward AD with npu_diou that does not support it.");
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> npu_lstm(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, const at::Tensor & seq_mask, const at::Tensor & h, const at::Tensor & c, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) {
  auto& input_ = unpack(input, "input", 0);
  auto& weight_ = unpack(weight, "weight", 1);
  auto& bias_ = unpack(bias, "bias", 2);
  auto& seq_mask_ = unpack(seq_mask, "seq_mask", 3);
  auto& h_ = unpack(h, "h", 4);
  auto& c_ = unpack(c, "c", 5);
  auto _any_requires_grad = compute_requires_grad( input, weight, bias, h, c );
  
  (void)_any_requires_grad;
  check_no_requires_grad(seq_mask, "seq_mask", "npu_lstm");
  std::shared_ptr<NpuLstmBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuLstmBackward0>(new NpuLstmBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias, h, c ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->weight_ = SavedVariable(weight, false);
    grad_fn->bias_ = SavedVariable(bias, false);
    grad_fn->h_ = SavedVariable(h, false);
    grad_fn->c_ = SavedVariable(c, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  at::Tensor result4;
  at::Tensor result5;
  at::Tensor result6;
  at::Tensor result7;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> bias__storage_saved =
    bias_.has_storage() ? c10::optional<Storage>(bias_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> bias__impl_saved;
  if (bias_.defined()) bias__impl_saved = bias_.getIntrusivePtr();
  c10::optional<Storage> seq_mask__storage_saved =
    seq_mask_.has_storage() ? c10::optional<Storage>(seq_mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> seq_mask__impl_saved;
  if (seq_mask_.defined()) seq_mask__impl_saved = seq_mask_.getIntrusivePtr();
  c10::optional<Storage> h__storage_saved =
    h_.has_storage() ? c10::optional<Storage>(h_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> h__impl_saved;
  if (h_.defined()) h__impl_saved = h_.getIntrusivePtr();
  c10::optional<Storage> c__storage_saved =
    c_.has_storage() ? c10::optional<Storage>(c_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> c__impl_saved;
  if (c_.defined()) c__impl_saved = c_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_lstm(input_, weight_, bias_, seq_mask_, h_, c_, has_biases, num_layers, dropout, train, bidirectional, batch_first, flag_seq, direction);
  })();
  std::tie(result0, result1, result2, result3, result4, result5, result6, result7) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (weight__storage_saved.has_value())
    AT_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved) AT_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (bias__storage_saved.has_value())
    AT_ASSERT(bias__storage_saved.value().is_alias_of(bias_.storage()));
  if (bias__impl_saved) AT_ASSERT(bias__impl_saved == bias_.getIntrusivePtr());
  if (seq_mask__storage_saved.has_value())
    AT_ASSERT(seq_mask__storage_saved.value().is_alias_of(seq_mask_.storage()));
  if (seq_mask__impl_saved) AT_ASSERT(seq_mask__impl_saved == seq_mask_.getIntrusivePtr());
  if (h__storage_saved.has_value())
    AT_ASSERT(h__storage_saved.value().is_alias_of(h_.storage()));
  if (h__impl_saved) AT_ASSERT(h__impl_saved == h_.getIntrusivePtr());
  if (c__storage_saved.has_value())
    AT_ASSERT(c__storage_saved.value().is_alias_of(c_.storage()));
  if (c__impl_saved) AT_ASSERT(c__impl_saved == c_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_lstm");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_lstm");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_lstm");
  if (result3.has_storage()) AT_ASSERT(result3.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result3.use_count() <= 1, "function: npu_lstm");
  if (result4.has_storage()) AT_ASSERT(result4.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result4.use_count() <= 1, "function: npu_lstm");
  if (result5.has_storage()) AT_ASSERT(result5.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result5.use_count() <= 1, "function: npu_lstm");
  if (result6.has_storage()) AT_ASSERT(result6.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result6.use_count() <= 1, "function: npu_lstm");
  if (result7.has_storage()) AT_ASSERT(result7.storage().use_count() == 1, "function: npu_lstm");
  AT_ASSERT(result7.use_count() <= 1, "function: npu_lstm");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_lstm");
  throw_error_for_complex_autograd(result1, "npu_lstm");
  throw_error_for_complex_autograd(result2, "npu_lstm");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(seq_mask) || isFwGradDefined(h) || isFwGradDefined(c)), "Trying to use forward AD with npu_lstm that does not support it.");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4_ = SavedVariable(result4, true);
    grad_fn->result5_ = SavedVariable(result5, true);
    grad_fn->result6_ = SavedVariable(result6, true);
    grad_fn->result7_ = SavedVariable(result7, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4), std::move(result5), std::move(result6), std::move(result7));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> npu_lstm_data(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & batch_sizes, const at::Tensor & weight, const at::Tensor & bias, const at::Tensor & seq_mask, const at::Tensor & h, const at::Tensor & c, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) {
  auto& input_ = unpack(input, "input", 0);
  auto& batch_sizes_ = unpack(batch_sizes, "batch_sizes", 1);
  auto& weight_ = unpack(weight, "weight", 2);
  auto& bias_ = unpack(bias, "bias", 3);
  auto& seq_mask_ = unpack(seq_mask, "seq_mask", 4);
  auto& h_ = unpack(h, "h", 5);
  auto& c_ = unpack(c, "c", 6);
  auto _any_requires_grad = compute_requires_grad( input, weight, bias, h, c );
  
  (void)_any_requires_grad;
  check_no_requires_grad(batch_sizes, "batch_sizes", "npu_lstm_data");
  check_no_requires_grad(seq_mask, "seq_mask", "npu_lstm_data");
  std::shared_ptr<NpuLstmDataBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuLstmDataBackward0>(new NpuLstmDataBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, weight, bias, h, c ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->batch_sizes_ = SavedVariable(batch_sizes, false);
    grad_fn->weight_ = SavedVariable(weight, false);
    grad_fn->bias_ = SavedVariable(bias, false);
    grad_fn->h_ = SavedVariable(h, false);
    grad_fn->c_ = SavedVariable(c, false);
    grad_fn->direction = direction;
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  at::Tensor result4;
  at::Tensor result5;
  at::Tensor result6;
  at::Tensor result7;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> batch_sizes__storage_saved =
    batch_sizes_.has_storage() ? c10::optional<Storage>(batch_sizes_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> batch_sizes__impl_saved;
  if (batch_sizes_.defined()) batch_sizes__impl_saved = batch_sizes_.getIntrusivePtr();
  c10::optional<Storage> weight__storage_saved =
    weight_.has_storage() ? c10::optional<Storage>(weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> weight__impl_saved;
  if (weight_.defined()) weight__impl_saved = weight_.getIntrusivePtr();
  c10::optional<Storage> bias__storage_saved =
    bias_.has_storage() ? c10::optional<Storage>(bias_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> bias__impl_saved;
  if (bias_.defined()) bias__impl_saved = bias_.getIntrusivePtr();
  c10::optional<Storage> seq_mask__storage_saved =
    seq_mask_.has_storage() ? c10::optional<Storage>(seq_mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> seq_mask__impl_saved;
  if (seq_mask_.defined()) seq_mask__impl_saved = seq_mask_.getIntrusivePtr();
  c10::optional<Storage> h__storage_saved =
    h_.has_storage() ? c10::optional<Storage>(h_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> h__impl_saved;
  if (h_.defined()) h__impl_saved = h_.getIntrusivePtr();
  c10::optional<Storage> c__storage_saved =
    c_.has_storage() ? c10::optional<Storage>(c_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> c__impl_saved;
  if (c_.defined()) c__impl_saved = c_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_lstm_data(input_, batch_sizes_, weight_, bias_, seq_mask_, h_, c_, has_biases, num_layers, dropout, train, bidirectional, batch_first, flag_seq, direction);
  })();
  std::tie(result0, result1, result2, result3, result4, result5, result6, result7) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (batch_sizes__storage_saved.has_value())
    AT_ASSERT(batch_sizes__storage_saved.value().is_alias_of(batch_sizes_.storage()));
  if (batch_sizes__impl_saved) AT_ASSERT(batch_sizes__impl_saved == batch_sizes_.getIntrusivePtr());
  if (weight__storage_saved.has_value())
    AT_ASSERT(weight__storage_saved.value().is_alias_of(weight_.storage()));
  if (weight__impl_saved) AT_ASSERT(weight__impl_saved == weight_.getIntrusivePtr());
  if (bias__storage_saved.has_value())
    AT_ASSERT(bias__storage_saved.value().is_alias_of(bias_.storage()));
  if (bias__impl_saved) AT_ASSERT(bias__impl_saved == bias_.getIntrusivePtr());
  if (seq_mask__storage_saved.has_value())
    AT_ASSERT(seq_mask__storage_saved.value().is_alias_of(seq_mask_.storage()));
  if (seq_mask__impl_saved) AT_ASSERT(seq_mask__impl_saved == seq_mask_.getIntrusivePtr());
  if (h__storage_saved.has_value())
    AT_ASSERT(h__storage_saved.value().is_alias_of(h_.storage()));
  if (h__impl_saved) AT_ASSERT(h__impl_saved == h_.getIntrusivePtr());
  if (c__storage_saved.has_value())
    AT_ASSERT(c__storage_saved.value().is_alias_of(c_.storage()));
  if (c__impl_saved) AT_ASSERT(c__impl_saved == c_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_lstm_data");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_lstm_data");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_lstm_data");
  if (result3.has_storage()) AT_ASSERT(result3.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result3.use_count() <= 1, "function: npu_lstm_data");
  if (result4.has_storage()) AT_ASSERT(result4.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result4.use_count() <= 1, "function: npu_lstm_data");
  if (result5.has_storage()) AT_ASSERT(result5.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result5.use_count() <= 1, "function: npu_lstm_data");
  if (result6.has_storage()) AT_ASSERT(result6.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result6.use_count() <= 1, "function: npu_lstm_data");
  if (result7.has_storage()) AT_ASSERT(result7.storage().use_count() == 1, "function: npu_lstm_data");
  AT_ASSERT(result7.use_count() <= 1, "function: npu_lstm_data");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_lstm_data");
  throw_error_for_complex_autograd(result1, "npu_lstm_data");
  throw_error_for_complex_autograd(result2, "npu_lstm_data");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(batch_sizes) || isFwGradDefined(weight) || isFwGradDefined(bias) || isFwGradDefined(seq_mask) || isFwGradDefined(h) || isFwGradDefined(c)), "Trying to use forward AD with npu_lstm_data that does not support it.");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4_ = SavedVariable(result4, true);
    grad_fn->result5_ = SavedVariable(result5, true);
    grad_fn->result6_ = SavedVariable(result6, true);
    grad_fn->result7_ = SavedVariable(result7, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4), std::move(result5), std::move(result6), std::move(result7));
}
::std::tuple<at::Tensor,at::Tensor> _dropout_with_byte_mask(c10::DispatchKeySet ks, const at::Tensor & self, double p) {
  auto& self_ = unpack(self, "self", 0);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  std::shared_ptr<DropoutWithByteMaskBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<DropoutWithByteMaskBackward0>(new DropoutWithByteMaskBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->p = p;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::_dropout_with_byte_mask(self_, p);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: _dropout_with_byte_mask");
  AT_ASSERT(result0.use_count() <= 1, "function: _dropout_with_byte_mask");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: _dropout_with_byte_mask");
  AT_ASSERT(result1.use_count() <= 1, "function: _dropout_with_byte_mask");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_dropout_with_byte_mask");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self)), "Trying to use forward AD with _dropout_with_byte_mask that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor> npu_dropout_with_add_softmax(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & x1, const at::Scalar & alpha, double prob, int64_t dim) {
  auto& self_ = unpack(self, "self", 0);
  auto& x1_ = unpack(x1, "x1", 1);
  auto _any_requires_grad = compute_requires_grad( self, x1 );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuDropoutWithAddSoftmaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuDropoutWithAddSoftmaxBackward0>(new NpuDropoutWithAddSoftmaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, x1 ));
    grad_fn->alpha = alpha;
    grad_fn->prob = prob;
    grad_fn->dim = dim;
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> x1__storage_saved =
    x1_.has_storage() ? c10::optional<Storage>(x1_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x1__impl_saved;
  if (x1_.defined()) x1__impl_saved = x1_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_dropout_with_add_softmax(self_, x1_, alpha, prob, dim);
  })();
  std::tie(result0, result1, result2) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (x1__storage_saved.has_value())
    AT_ASSERT(x1__storage_saved.value().is_alias_of(x1_.storage()));
  if (x1__impl_saved) AT_ASSERT(x1__impl_saved == x1_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_dropout_with_add_softmax");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_dropout_with_add_softmax");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_dropout_with_add_softmax");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_dropout_with_add_softmax");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_dropout_with_add_softmax");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_dropout_with_add_softmax");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result2, "npu_dropout_with_add_softmax");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(x1)), "Trying to use forward AD with npu_dropout_with_add_softmax that does not support it.");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2));
}
at::Tensor npu_scaled_masked_softmax(c10::DispatchKeySet ks, const at::Tensor & x, const at::Tensor & mask, const at::Scalar & scale, bool fixed_triu_mask) {
  auto& x_ = unpack(x, "x", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  auto _any_requires_grad = compute_requires_grad( x );
  
  (void)_any_requires_grad;
  check_no_requires_grad(mask, "mask", "npu_scaled_masked_softmax");
  std::shared_ptr<NpuScaledMaskedSoftmaxBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuScaledMaskedSoftmaxBackward0>(new NpuScaledMaskedSoftmaxBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( x ));
    grad_fn->mask_ = SavedVariable(mask, false);
    grad_fn->scale = scale;
    grad_fn->fixed_triu_mask = fixed_triu_mask;
  }
  #ifndef NDEBUG
  c10::optional<Storage> x__storage_saved =
    x_.has_storage() ? c10::optional<Storage>(x_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> x__impl_saved;
  if (x_.defined()) x__impl_saved = x_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_scaled_masked_softmax(x_, mask_, scale, fixed_triu_mask);
  })();
  auto result = std::move(_tmp);
  #ifndef NDEBUG
  if (x__storage_saved.has_value())
    AT_ASSERT(x__storage_saved.value().is_alias_of(x_.storage()));
  if (x__impl_saved) AT_ASSERT(x__impl_saved == x_.getIntrusivePtr());
  if (mask__storage_saved.has_value())
    AT_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved) AT_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result.has_storage()) AT_ASSERT(result.storage().use_count() == 1, "function: npu_scaled_masked_softmax");
  AT_ASSERT(result.use_count() <= 1, "function: npu_scaled_masked_softmax");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  throw_error_for_complex_autograd(result, "npu_scaled_masked_softmax");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(x) || isFwGradDefined(mask)), "Trying to use forward AD with npu_scaled_masked_softmax that does not support it.");
  if (grad_fn) {
    grad_fn->result_ = SavedVariable(result, true);
  }
  return result;
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> npu_multi_head_attention(c10::DispatchKeySet ks, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, const at::Tensor & query_weight, const at::Tensor & key_weight, const at::Tensor & value_weight, const at::Tensor & attn_mask, const at::Tensor & out_proj_weight, const c10::optional<at::Tensor> & query_bias, const c10::optional<at::Tensor> & key_bias, const c10::optional<at::Tensor> & value_bias, const c10::optional<at::Tensor> & out_proj_bias, const c10::optional<at::Tensor> & dropout_mask, int64_t attn_head_num, int64_t attn_dim_per_head, int64_t src_len, int64_t tgt_len, double dropout_prob, bool softmax_use_float) {
  auto& query_ = unpack(query, "query", 0);
  auto& key_ = unpack(key, "key", 1);
  auto& value_ = unpack(value, "value", 2);
  auto& query_weight_ = unpack(query_weight, "query_weight", 3);
  auto& key_weight_ = unpack(key_weight, "key_weight", 4);
  auto& value_weight_ = unpack(value_weight, "value_weight", 5);
  auto& attn_mask_ = unpack(attn_mask, "attn_mask", 6);
  auto& out_proj_weight_ = unpack(out_proj_weight, "out_proj_weight", 7);
  auto _any_requires_grad = compute_requires_grad( query, key, value, query_weight, key_weight, value_weight, out_proj_weight, query_bias, key_bias, value_bias, out_proj_bias );
  
  (void)_any_requires_grad;
  check_no_requires_grad(attn_mask, "attn_mask", "npu_multi_head_attention");
  check_no_requires_grad(dropout_mask, "dropout_mask", "npu_multi_head_attention");
  std::shared_ptr<NpuMultiHeadAttentionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuMultiHeadAttentionBackward0>(new NpuMultiHeadAttentionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( query, key, value, query_weight, key_weight, value_weight, out_proj_weight, query_bias, key_bias, value_bias, out_proj_bias ));
    grad_fn->query_ = SavedVariable(query, false);
    grad_fn->key_ = SavedVariable(key, false);
    grad_fn->value_ = SavedVariable(value, false);
    grad_fn->query_weight_ = SavedVariable(query_weight, false);
    grad_fn->key_weight_ = SavedVariable(key_weight, false);
    grad_fn->value_weight_ = SavedVariable(value_weight, false);
    grad_fn->out_proj_weight_ = SavedVariable(out_proj_weight, false);
    grad_fn->query_bias_ = SavedVariable(query_bias, false);
    grad_fn->key_bias_ = SavedVariable(key_bias, false);
    grad_fn->value_bias_ = SavedVariable(value_bias, false);
    grad_fn->out_proj_bias_ = SavedVariable(out_proj_bias, false);
    grad_fn->attn_head_num = attn_head_num;
    grad_fn->attn_dim_per_head = attn_dim_per_head;
    grad_fn->src_len = src_len;
    grad_fn->tgt_len = tgt_len;
    grad_fn->dropout_prob = dropout_prob;
    grad_fn->softmax_use_float = softmax_use_float;
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  at::Tensor result4;
  at::Tensor result5;
  at::Tensor result6;
  at::Tensor result7;
  #ifndef NDEBUG
  c10::optional<Storage> query__storage_saved =
    query_.has_storage() ? c10::optional<Storage>(query_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query__impl_saved;
  if (query_.defined()) query__impl_saved = query_.getIntrusivePtr();
  c10::optional<Storage> key__storage_saved =
    key_.has_storage() ? c10::optional<Storage>(key_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key__impl_saved;
  if (key_.defined()) key__impl_saved = key_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  c10::optional<Storage> query_weight__storage_saved =
    query_weight_.has_storage() ? c10::optional<Storage>(query_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query_weight__impl_saved;
  if (query_weight_.defined()) query_weight__impl_saved = query_weight_.getIntrusivePtr();
  c10::optional<Storage> key_weight__storage_saved =
    key_weight_.has_storage() ? c10::optional<Storage>(key_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key_weight__impl_saved;
  if (key_weight_.defined()) key_weight__impl_saved = key_weight_.getIntrusivePtr();
  c10::optional<Storage> value_weight__storage_saved =
    value_weight_.has_storage() ? c10::optional<Storage>(value_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value_weight__impl_saved;
  if (value_weight_.defined()) value_weight__impl_saved = value_weight_.getIntrusivePtr();
  c10::optional<Storage> attn_mask__storage_saved =
    attn_mask_.has_storage() ? c10::optional<Storage>(attn_mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> attn_mask__impl_saved;
  if (attn_mask_.defined()) attn_mask__impl_saved = attn_mask_.getIntrusivePtr();
  c10::optional<Storage> out_proj_weight__storage_saved =
    out_proj_weight_.has_storage() ? c10::optional<Storage>(out_proj_weight_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> out_proj_weight__impl_saved;
  if (out_proj_weight_.defined()) out_proj_weight__impl_saved = out_proj_weight_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_multi_head_attention(query_, key_, value_, query_weight_, key_weight_, value_weight_, attn_mask_, out_proj_weight_, query_bias, key_bias, value_bias, out_proj_bias, dropout_mask, attn_head_num, attn_dim_per_head, src_len, tgt_len, dropout_prob, softmax_use_float);
  })();
  std::tie(result0, result1, result2, result3, result4, result5, result6, result7) = std::move(_tmp);
  #ifndef NDEBUG
  if (query__storage_saved.has_value())
    AT_ASSERT(query__storage_saved.value().is_alias_of(query_.storage()));
  if (query__impl_saved) AT_ASSERT(query__impl_saved == query_.getIntrusivePtr());
  if (key__storage_saved.has_value())
    AT_ASSERT(key__storage_saved.value().is_alias_of(key_.storage()));
  if (key__impl_saved) AT_ASSERT(key__impl_saved == key_.getIntrusivePtr());
  if (value__storage_saved.has_value())
    AT_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved) AT_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (query_weight__storage_saved.has_value())
    AT_ASSERT(query_weight__storage_saved.value().is_alias_of(query_weight_.storage()));
  if (query_weight__impl_saved) AT_ASSERT(query_weight__impl_saved == query_weight_.getIntrusivePtr());
  if (key_weight__storage_saved.has_value())
    AT_ASSERT(key_weight__storage_saved.value().is_alias_of(key_weight_.storage()));
  if (key_weight__impl_saved) AT_ASSERT(key_weight__impl_saved == key_weight_.getIntrusivePtr());
  if (value_weight__storage_saved.has_value())
    AT_ASSERT(value_weight__storage_saved.value().is_alias_of(value_weight_.storage()));
  if (value_weight__impl_saved) AT_ASSERT(value_weight__impl_saved == value_weight_.getIntrusivePtr());
  if (attn_mask__storage_saved.has_value())
    AT_ASSERT(attn_mask__storage_saved.value().is_alias_of(attn_mask_.storage()));
  if (attn_mask__impl_saved) AT_ASSERT(attn_mask__impl_saved == attn_mask_.getIntrusivePtr());
  if (out_proj_weight__storage_saved.has_value())
    AT_ASSERT(out_proj_weight__storage_saved.value().is_alias_of(out_proj_weight_.storage()));
  if (out_proj_weight__impl_saved) AT_ASSERT(out_proj_weight__impl_saved == out_proj_weight_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_multi_head_attention");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_multi_head_attention");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_multi_head_attention");
  if (result3.has_storage()) AT_ASSERT(result3.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result3.use_count() <= 1, "function: npu_multi_head_attention");
  if (result4.has_storage()) AT_ASSERT(result4.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result4.use_count() <= 1, "function: npu_multi_head_attention");
  if (result5.has_storage()) AT_ASSERT(result5.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result5.use_count() <= 1, "function: npu_multi_head_attention");
  if (result6.has_storage()) AT_ASSERT(result6.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result6.use_count() <= 1, "function: npu_multi_head_attention");
  if (result7.has_storage()) AT_ASSERT(result7.storage().use_count() == 1, "function: npu_multi_head_attention");
  AT_ASSERT(result7.use_count() <= 1, "function: npu_multi_head_attention");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_multi_head_attention");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(query) || isFwGradDefined(key) || isFwGradDefined(value) || isFwGradDefined(query_weight) || isFwGradDefined(key_weight) || isFwGradDefined(value_weight) || isFwGradDefined(attn_mask) || isFwGradDefined(out_proj_weight) || isFwGradDefined(query_bias) || isFwGradDefined(key_bias) || isFwGradDefined(value_bias) || isFwGradDefined(out_proj_bias) || isFwGradDefined(dropout_mask)), "Trying to use forward AD with npu_multi_head_attention that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4_ = SavedVariable(result4, true);
    grad_fn->result5_ = SavedVariable(result5, true);
    grad_fn->result6_ = SavedVariable(result6, true);
    grad_fn->result7_ = SavedVariable(result7, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4), std::move(result5), std::move(result6), std::move(result7));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t,int64_t,int64_t> npu_fusion_attention(c10::DispatchKeySet ks, const at::Tensor & query, const at::Tensor & key, const at::Tensor & value, int64_t head_num, c10::string_view input_layout, const c10::optional<at::Tensor> & pse, const c10::optional<at::Tensor> & padding_mask, const c10::optional<at::Tensor> & atten_mask, double scale, double keep_prob, int64_t pre_tockens, int64_t next_tockens, int64_t inner_precise, bool gen_mask_parallel, bool sync) {
  auto& query_ = unpack(query, "query", 0);
  auto& key_ = unpack(key, "key", 1);
  auto& value_ = unpack(value, "value", 2);
  auto _any_requires_grad = compute_requires_grad( query, key, value, pse );
  
  (void)_any_requires_grad;
  check_no_requires_grad(padding_mask, "padding_mask", "npu_fusion_attention");
  check_no_requires_grad(atten_mask, "atten_mask", "npu_fusion_attention");
  std::shared_ptr<NpuFusionAttentionBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuFusionAttentionBackward0>(new NpuFusionAttentionBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( query, key, value, pse ));
    grad_fn->query_ = SavedVariable(query, false);
    grad_fn->key_ = SavedVariable(key, false);
    grad_fn->value_ = SavedVariable(value, false);
    grad_fn->head_num = head_num;
    grad_fn->input_layout = std::string(input_layout);
    grad_fn->pse_ = SavedVariable(pse, false);
    grad_fn->padding_mask_ = SavedVariable(padding_mask, false);
    grad_fn->atten_mask_ = SavedVariable(atten_mask, false);
    grad_fn->scale = scale;
    grad_fn->keep_prob = keep_prob;
    grad_fn->pre_tockens = pre_tockens;
    grad_fn->next_tockens = next_tockens;
    grad_fn->inner_precise = inner_precise;
    grad_fn->gen_mask_parallel = gen_mask_parallel;
    grad_fn->sync = sync;
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  int64_t result4;
  int64_t result5;
  int64_t result6;
  #ifndef NDEBUG
  c10::optional<Storage> query__storage_saved =
    query_.has_storage() ? c10::optional<Storage>(query_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> query__impl_saved;
  if (query_.defined()) query__impl_saved = query_.getIntrusivePtr();
  c10::optional<Storage> key__storage_saved =
    key_.has_storage() ? c10::optional<Storage>(key_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> key__impl_saved;
  if (key_.defined()) key__impl_saved = key_.getIntrusivePtr();
  c10::optional<Storage> value__storage_saved =
    value_.has_storage() ? c10::optional<Storage>(value_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> value__impl_saved;
  if (value_.defined()) value__impl_saved = value_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_fusion_attention(query_, key_, value_, head_num, input_layout, pse, padding_mask, atten_mask, scale, keep_prob, pre_tockens, next_tockens, inner_precise, gen_mask_parallel, sync);
  })();
  std::tie(result0, result1, result2, result3, result4, result5, result6) = std::move(_tmp);
  #ifndef NDEBUG
  if (query__storage_saved.has_value())
    AT_ASSERT(query__storage_saved.value().is_alias_of(query_.storage()));
  if (query__impl_saved) AT_ASSERT(query__impl_saved == query_.getIntrusivePtr());
  if (key__storage_saved.has_value())
    AT_ASSERT(key__storage_saved.value().is_alias_of(key_.storage()));
  if (key__impl_saved) AT_ASSERT(key__impl_saved == key_.getIntrusivePtr());
  if (value__storage_saved.has_value())
    AT_ASSERT(value__storage_saved.value().is_alias_of(value_.storage()));
  if (value__impl_saved) AT_ASSERT(value__impl_saved == value_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_fusion_attention");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_fusion_attention");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_fusion_attention");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_fusion_attention");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_fusion_attention");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_fusion_attention");
  if (result3.has_storage()) AT_ASSERT(result3.storage().use_count() == 1, "function: npu_fusion_attention");
  AT_ASSERT(result3.use_count() <= 1, "function: npu_fusion_attention");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2, result3 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_fusion_attention");
  throw_error_for_complex_autograd(result1, "npu_fusion_attention");
  throw_error_for_complex_autograd(result2, "npu_fusion_attention");
  throw_error_for_complex_autograd(result3, "npu_fusion_attention");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(query) || isFwGradDefined(key) || isFwGradDefined(value) || isFwGradDefined(pse) || isFwGradDefined(padding_mask) || isFwGradDefined(atten_mask)), "Trying to use forward AD with npu_fusion_attention that does not support it.");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4 = result4;
    grad_fn->result5 = result5;
    grad_fn->result6 = result6;
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4), std::move(result5), std::move(result6));
}
::std::tuple<at::Tensor,at::Tensor> npu_dropout_do_mask(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & mask, double p) {
  auto& self_ = unpack(self, "self", 0);
  auto& mask_ = unpack(mask, "mask", 1);
  auto _any_requires_grad = compute_requires_grad( self );
  
  (void)_any_requires_grad;
  check_no_requires_grad(mask, "mask", "npu_dropout_do_mask");
  std::shared_ptr<NpuDropoutDoMaskBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuDropoutDoMaskBackward0>(new NpuDropoutDoMaskBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self ));
    grad_fn->p = p;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> mask__storage_saved =
    mask_.has_storage() ? c10::optional<Storage>(mask_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> mask__impl_saved;
  if (mask_.defined()) mask__impl_saved = mask_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_dropout_do_mask(self_, mask_, p);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (mask__storage_saved.has_value())
    AT_ASSERT(mask__storage_saved.value().is_alias_of(mask_.storage()));
  if (mask__impl_saved) AT_ASSERT(mask__impl_saved == mask_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_dropout_do_mask");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_dropout_do_mask");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_dropout_do_mask");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_dropout_do_mask");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_dropout_do_mask");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(mask)), "Trying to use forward AD with npu_dropout_do_mask that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> npu_lstm_cell(c10::DispatchKeySet ks, const at::Tensor & input, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & h, const at::Tensor & c, const c10::optional<at::Tensor> & bias) {
  auto& input_ = unpack(input, "input", 0);
  auto& w_ih_ = unpack(w_ih, "w_ih", 1);
  auto& w_hh_ = unpack(w_hh, "w_hh", 2);
  auto& h_ = unpack(h, "h", 3);
  auto& c_ = unpack(c, "c", 4);
  auto _any_requires_grad = compute_requires_grad( input, w_ih, w_hh, h, c, bias );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuLstmCellBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuLstmCellBackward0>(new NpuLstmCellBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( input, w_ih, w_hh, h, c, bias ));
    grad_fn->input_ = SavedVariable(input, false);
    grad_fn->w_ih_ = SavedVariable(w_ih, false);
    grad_fn->w_hh_ = SavedVariable(w_hh, false);
    grad_fn->h_ = SavedVariable(h, false);
    grad_fn->c_ = SavedVariable(c, false);
  }
  at::Tensor result0;
  at::Tensor result1;
  at::Tensor result2;
  at::Tensor result3;
  at::Tensor result4;
  at::Tensor result5;
  at::Tensor result6;
  at::Tensor result7;
  #ifndef NDEBUG
  c10::optional<Storage> input__storage_saved =
    input_.has_storage() ? c10::optional<Storage>(input_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> input__impl_saved;
  if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();
  c10::optional<Storage> w_ih__storage_saved =
    w_ih_.has_storage() ? c10::optional<Storage>(w_ih_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> w_ih__impl_saved;
  if (w_ih_.defined()) w_ih__impl_saved = w_ih_.getIntrusivePtr();
  c10::optional<Storage> w_hh__storage_saved =
    w_hh_.has_storage() ? c10::optional<Storage>(w_hh_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> w_hh__impl_saved;
  if (w_hh_.defined()) w_hh__impl_saved = w_hh_.getIntrusivePtr();
  c10::optional<Storage> h__storage_saved =
    h_.has_storage() ? c10::optional<Storage>(h_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> h__impl_saved;
  if (h_.defined()) h__impl_saved = h_.getIntrusivePtr();
  c10::optional<Storage> c__storage_saved =
    c_.has_storage() ? c10::optional<Storage>(c_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> c__impl_saved;
  if (c_.defined()) c__impl_saved = c_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::npu_lstm_cell(input_, w_ih_, w_hh_, h_, c_, bias);
  })();
  std::tie(result0, result1, result2, result3, result4, result5, result6, result7) = std::move(_tmp);
  #ifndef NDEBUG
  if (input__storage_saved.has_value())
    AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));
  if (input__impl_saved) AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());
  if (w_ih__storage_saved.has_value())
    AT_ASSERT(w_ih__storage_saved.value().is_alias_of(w_ih_.storage()));
  if (w_ih__impl_saved) AT_ASSERT(w_ih__impl_saved == w_ih_.getIntrusivePtr());
  if (w_hh__storage_saved.has_value())
    AT_ASSERT(w_hh__storage_saved.value().is_alias_of(w_hh_.storage()));
  if (w_hh__impl_saved) AT_ASSERT(w_hh__impl_saved == w_hh_.getIntrusivePtr());
  if (h__storage_saved.has_value())
    AT_ASSERT(h__storage_saved.value().is_alias_of(h_.storage()));
  if (h__impl_saved) AT_ASSERT(h__impl_saved == h_.getIntrusivePtr());
  if (c__storage_saved.has_value())
    AT_ASSERT(c__storage_saved.value().is_alias_of(c_.storage()));
  if (c__impl_saved) AT_ASSERT(c__impl_saved == c_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result0.use_count() <= 1, "function: npu_lstm_cell");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result1.use_count() <= 1, "function: npu_lstm_cell");
  if (result2.has_storage()) AT_ASSERT(result2.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result2.use_count() <= 1, "function: npu_lstm_cell");
  if (result3.has_storage()) AT_ASSERT(result3.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result3.use_count() <= 1, "function: npu_lstm_cell");
  if (result4.has_storage()) AT_ASSERT(result4.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result4.use_count() <= 1, "function: npu_lstm_cell");
  if (result5.has_storage()) AT_ASSERT(result5.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result5.use_count() <= 1, "function: npu_lstm_cell");
  if (result6.has_storage()) AT_ASSERT(result6.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result6.use_count() <= 1, "function: npu_lstm_cell");
  if (result7.has_storage()) AT_ASSERT(result7.storage().use_count() == 1, "function: npu_lstm_cell");
  AT_ASSERT(result7.use_count() <= 1, "function: npu_lstm_cell");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0, result1, result2 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "npu_lstm_cell");
  throw_error_for_complex_autograd(result1, "npu_lstm_cell");
  throw_error_for_complex_autograd(result2, "npu_lstm_cell");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(input) || isFwGradDefined(w_ih) || isFwGradDefined(w_hh) || isFwGradDefined(h) || isFwGradDefined(c) || isFwGradDefined(bias)), "Trying to use forward AD with npu_lstm_cell that does not support it.");
  if (grad_fn) {
    grad_fn->result0_ = SavedVariable(result0, true);
    grad_fn->result1_ = SavedVariable(result1, true);
    grad_fn->result2_ = SavedVariable(result2, true);
    grad_fn->result3_ = SavedVariable(result3, true);
    grad_fn->result4_ = SavedVariable(result4, true);
    grad_fn->result5_ = SavedVariable(result5, true);
    grad_fn->result6_ = SavedVariable(result6, true);
    grad_fn->result7_ = SavedVariable(result7, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1), std::move(result2), std::move(result3), std::move(result4), std::move(result5), std::move(result6), std::move(result7));
}
::std::tuple<at::Tensor,at::Tensor> _npu_ciou(c10::DispatchKeySet ks, const at::Tensor & self, const at::Tensor & gtboxes, bool trans, bool is_cross, int64_t mode, bool atan_sub_flag) {
  auto& self_ = unpack(self, "self", 0);
  auto& gtboxes_ = unpack(gtboxes, "gtboxes", 1);
  auto _any_requires_grad = compute_requires_grad( self, gtboxes );
  
  (void)_any_requires_grad;
  std::shared_ptr<NpuCiouBackward0> grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr<NpuCiouBackward0>(new NpuCiouBackward0(), deleteNode);
    grad_fn->set_next_edges(collect_next_edges( self, gtboxes ));
    grad_fn->self_ = SavedVariable(self, false);
    grad_fn->gtboxes_ = SavedVariable(gtboxes, false);
    grad_fn->trans = trans;
    grad_fn->is_cross = is_cross;
    grad_fn->mode = mode;
  }
  at::Tensor result0;
  at::Tensor result1;
  #ifndef NDEBUG
  c10::optional<Storage> self__storage_saved =
    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> self__impl_saved;
  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();
  c10::optional<Storage> gtboxes__storage_saved =
    gtboxes_.has_storage() ? c10::optional<Storage>(gtboxes_.storage()) : c10::nullopt;
  c10::intrusive_ptr<TensorImpl> gtboxes__impl_saved;
  if (gtboxes_.defined()) gtboxes__impl_saved = gtboxes_.getIntrusivePtr();
  #endif
  auto _tmp = ([&]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return op_plugin::_npu_ciou(self_, gtboxes_, trans, is_cross, mode, atan_sub_flag);
  })();
  std::tie(result0, result1) = std::move(_tmp);
  #ifndef NDEBUG
  if (self__storage_saved.has_value())
    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));
  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());
  if (gtboxes__storage_saved.has_value())
    AT_ASSERT(gtboxes__storage_saved.value().is_alias_of(gtboxes_.storage()));
  if (gtboxes__impl_saved) AT_ASSERT(gtboxes__impl_saved == gtboxes_.getIntrusivePtr());
  if (result0.has_storage()) AT_ASSERT(result0.storage().use_count() == 1, "function: _npu_ciou");
  AT_ASSERT(result0.use_count() <= 1, "function: _npu_ciou");
  if (result1.has_storage()) AT_ASSERT(result1.storage().use_count() == 1, "function: _npu_ciou");
  AT_ASSERT(result1.use_count() <= 1, "function: _npu_ciou");
  #endif
  if (grad_fn) {
      set_history(flatten_tensor_args( result0 ), grad_fn);
  }
  throw_error_for_complex_autograd(result0, "_npu_ciou");
  TORCH_CHECK_NOT_IMPLEMENTED(!(isFwGradDefined(self) || isFwGradDefined(gtboxes)), "Trying to use forward AD with _npu_ciou that does not support it.");
  if (grad_fn) {
    grad_fn->result1_ = SavedVariable(result1, true);
  }
  return std::make_tuple(std::move(result0), std::move(result1));
}
} // namespace VariableType

namespace {

TORCH_LIBRARY_IMPL(aten, AutogradXLA, m) {
  m.impl("_npu_format_cast",
         TORCH_FN(VariableType::_npu_format_cast)
  );
  m.impl("fast_gelu",
         TORCH_FN(VariableType::fast_gelu)
  );
  m.impl("npu_fused_attention_score_fwd",
         TORCH_FN(VariableType::npu_fused_attention_score_fwd)
  );
  m.impl("npu_rotary_mul",
         TORCH_FN(VariableType::npu_rotary_mul)
  );
  m.impl("npu_convolution",
         TORCH_FN(VariableType::npu_convolution)
  );
  m.impl("npu_convolution_transpose",
         TORCH_FN(VariableType::npu_convolution_transpose)
  );
  m.impl("npu_confusion_transpose",
         TORCH_FN(VariableType::npu_confusion_transpose)
  );
  m.impl("npu_ps_roi_pooling",
         TORCH_FN(VariableType::npu_ps_roi_pooling)
  );
  m.impl("npu_linear",
         TORCH_FN(VariableType::npu_linear)
  );
  m.impl("_npu_dropout",
         TORCH_FN(VariableType::_npu_dropout)
  );
  m.impl("npu_softmax_cross_entropy_with_logits",
         TORCH_FN(VariableType::npu_softmax_cross_entropy_with_logits)
  );
  m.impl("npu_max.dim",
         TORCH_FN(VariableType::npu_max_dim)
  );
  m.impl("npu_bmmV2",
         TORCH_FN(VariableType::npu_bmmV2)
  );
  m.impl("npu_dtype_cast",
         TORCH_FN(VariableType::npu_dtype_cast)
  );
  m.impl("npu_silu",
         TORCH_FN(VariableType::npu_silu)
  );
  m.impl("npu_silu_",
         TORCH_FN(VariableType::npu_silu_)
  );
  m.impl("npu_gru",
         TORCH_FN(VariableType::npu_gru)
  );
  m.impl("npu_mish",
         TORCH_FN(VariableType::npu_mish)
  );
  m.impl("npu_min.dim",
         TORCH_FN(VariableType::npu_min_dim)
  );
  m.impl("npu_deformable_conv2d",
         TORCH_FN(VariableType::npu_deformable_conv2d)
  );
  m.impl("npu_giou",
         TORCH_FN(VariableType::npu_giou)
  );
  m.impl("npu_diou",
         TORCH_FN(VariableType::npu_diou)
  );
  m.impl("npu_lstm",
         TORCH_FN(VariableType::npu_lstm)
  );
  m.impl("npu_lstm_data",
         TORCH_FN(VariableType::npu_lstm_data)
  );
  m.impl("_dropout_with_byte_mask",
         TORCH_FN(VariableType::_dropout_with_byte_mask)
  );
  m.impl("npu_dropout_with_add_softmax",
         TORCH_FN(VariableType::npu_dropout_with_add_softmax)
  );
  m.impl("npu_scaled_masked_softmax",
         TORCH_FN(VariableType::npu_scaled_masked_softmax)
  );
  m.impl("npu_multi_head_attention",
         TORCH_FN(VariableType::npu_multi_head_attention)
  );
  m.impl("npu_fusion_attention",
         TORCH_FN(VariableType::npu_fusion_attention)
  );
  m.impl("npu_dropout_do_mask",
         TORCH_FN(VariableType::npu_dropout_do_mask)
  );
  m.impl("npu_lstm_cell",
         TORCH_FN(VariableType::npu_lstm_cell)
  );
  m.impl("_npu_ciou",
         TORCH_FN(VariableType::_npu_ciou)
  );
}

}

}} // namespace at_npu::autograd
