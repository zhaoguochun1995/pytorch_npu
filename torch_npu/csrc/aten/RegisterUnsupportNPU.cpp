// Copyright (c) 2020 Huawei Technologies Co., Ltd
// Copyright (c) 2019, Facebook CORPORATION.
// All rights reserved.
//
// Licensed under the BSD 3-Clause License  (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// https://opensource.org/licenses/BSD-3-Clause
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#ifndef __NPU_STDC_FORMAT_MACROS
#define __NPU_STDC_FORMAT_MACROS
#endif

// @generated by tools/codegen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/MetaFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/Functions.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <torch/library.h>

#include "torch_npu/csrc/profiler/utils.h"

#include "torch_npu/csrc/aten/NPUNativeFunctions.h"


namespace at {

// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {


void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}

namespace {

at::Tensor wrapper___conj(const at::Tensor & self) {
    TORCH_CHECK(false, "_conj is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conj(const at::Tensor & self) {
    TORCH_CHECK(false, "conj is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper___conj_physical(const at::Tensor & self) {
    TORCH_CHECK(false, "_conj_physical is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conj_physical(const at::Tensor & self) {
    TORCH_CHECK(false, "conj_physical is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_conj_physical_out(const at::Tensor & self, at::Tensor & out) {
    TORCH_CHECK(false, "conj_physical.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__conj_physical_(at::Tensor & self) {
    TORCH_CHECK(false, "conj_physical_ is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Tensor_isin(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
    TORCH_CHECK(false, "isin.Tensor_Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Tensor_out_isin_out(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
    TORCH_CHECK(false, "isin.Tensor_Tensor_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logdet(const at::Tensor & self) {
    TORCH_CHECK(false, "logdet is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_Tensor_frexp(const at::Tensor & self) {
    TORCH_CHECK(false, "frexp.Tensor is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_Tensor_out_frexp_out(const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
    TORCH_CHECK(false, "frexp.Tensor_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_bitwise_left_shift(const at::Tensor & self, const at::Tensor & other) {
    TORCH_CHECK(false, "bitwise_left_shift.Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_bitwise_left_shift_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    TORCH_CHECK(false, "bitwise_left_shift.Tensor_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_bitwise_left_shift_(at::Tensor & self, const at::Tensor & other) {
    TORCH_CHECK(false, "bitwise_left_shift_.Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Scalar_bitwise_left_shift(const at::Tensor & self, const at::Scalar & other) {
    TORCH_CHECK(false, "bitwise_left_shift.Tensor_Scalar is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Scalar_out_bitwise_left_shift_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    TORCH_CHECK(false, "bitwise_left_shift.Tensor_Scalar_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Scalar_bitwise_left_shift_(at::Tensor & self, const at::Scalar & other) {
    TORCH_CHECK(false, "bitwise_left_shift_.Tensor_Scalar is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_Tensor_bitwise_left_shift(const at::Scalar & self, const at::Tensor & other) {
    TORCH_CHECK(false, "bitwise_left_shift.Scalar_Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_bitwise_right_shift(const at::Tensor & self, const at::Tensor & other) {
    TORCH_CHECK(false, "bitwise_right_shift.Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_bitwise_right_shift_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    TORCH_CHECK(false, "bitwise_right_shift.Tensor_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_bitwise_right_shift_(at::Tensor & self, const at::Tensor & other) {
    TORCH_CHECK(false, "bitwise_right_shift_.Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Scalar_bitwise_right_shift(const at::Tensor & self, const at::Scalar & other) {
    TORCH_CHECK(false, "bitwise_right_shift.Tensor_Scalar is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Scalar_out_bitwise_right_shift_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    TORCH_CHECK(false, "bitwise_right_shift.Tensor_Scalar_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Scalar_bitwise_right_shift_(at::Tensor & self, const at::Scalar & other) {
    TORCH_CHECK(false, "bitwise_right_shift_.Tensor_Scalar is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_Tensor_bitwise_right_shift(const at::Scalar & self, const at::Tensor & other) {
    TORCH_CHECK(false, "bitwise_right_shift.Scalar_Tensor is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_solve_triangular(const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular) {
    TORCH_CHECK(false, "linalg_solve_triangular is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_solve_triangular_out(const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular, at::Tensor & out) {
    TORCH_CHECK(false, "linalg_solve_triangular.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cholesky(const at::Tensor & self, bool upper) {
    TORCH_CHECK(false, "cholesky is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_cholesky_out(const at::Tensor & self, bool upper, at::Tensor & out) {
    TORCH_CHECK(false, "cholesky.out is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__geqrf(const at::Tensor & self) {
    TORCH_CHECK(false, "geqrf is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_a_geqrf_out(const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
    TORCH_CHECK(false, "geqrf.a is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__lu_solve(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
    TORCH_CHECK(false, "lu_solve is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_lu_solve_out(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
    TORCH_CHECK(false, "lu_solve.out is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__lu_unpack(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
    TORCH_CHECK(false, "lu_unpack is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_out_lu_unpack_out(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
    TORCH_CHECK(false, "lu_unpack.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_entr(const at::Tensor & self) {
    TORCH_CHECK(false, "special_entr is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_entr_out(const at::Tensor & self, at::Tensor & out) {
    TORCH_CHECK(false, "special_entr.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_erfcx(const at::Tensor & self) {
    TORCH_CHECK(false, "special_erfcx is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_erfcx_out(const at::Tensor & self, at::Tensor & out) {
    TORCH_CHECK(false, "special_erfcx.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_zeta(const at::Tensor & self, const at::Tensor & other) {
    TORCH_CHECK(false, "special_zeta is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_zeta_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    TORCH_CHECK(false, "special_zeta.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_self_scalar_special_zeta(const at::Scalar & self, const at::Tensor & other) {
    TORCH_CHECK(false, "special_zeta.self_scalar is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_self_scalar_out_special_zeta_out(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    TORCH_CHECK(false, "special_zeta.self_scalar_out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_scalar_special_zeta(const at::Tensor & self, const at::Scalar & other) {
    TORCH_CHECK(false, "special_zeta.other_scalar is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_other_scalar_out_special_zeta_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    TORCH_CHECK(false, "special_zeta.other_scalar_out is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__linalg_cholesky_ex(const at::Tensor & self, bool upper, bool check_errors) {
    TORCH_CHECK(false, "linalg_cholesky_ex is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_L_linalg_cholesky_ex_out(const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
    TORCH_CHECK(false, "linalg_cholesky_ex.L is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__linalg_lu_factor_ex(const at::Tensor & A, bool pivot, bool check_errors) {
    TORCH_CHECK(false, "linalg_lu_factor_ex is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_out_linalg_lu_factor_ex_out(const at::Tensor & A, bool pivot, bool check_errors, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
    TORCH_CHECK(false, "linalg_lu_factor_ex.out is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_det(const at::Tensor & self) {
    TORCH_CHECK(false, "linalg_det is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_det_out(const at::Tensor & self, at::Tensor & out) {
    TORCH_CHECK(false, "linalg_det.out is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper___det_lu_based_helper(const at::Tensor & self) {
    TORCH_CHECK(false, "_det_lu_based_helper is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper__linalg_lstsq(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
    TORCH_CHECK(false, "linalg_lstsq is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> wrapper_out_linalg_lstsq_out(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
    TORCH_CHECK(false, "linalg_lstsq.out is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__linalg_eig(const at::Tensor & self) {
    TORCH_CHECK(false, "linalg_eig is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_out_linalg_eig_out(const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
    TORCH_CHECK(false, "linalg_eig.out is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__linalg_eigh(const at::Tensor & self, c10::string_view UPLO) {
    TORCH_CHECK(false, "linalg_eigh is unsupported!");
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_eigvals_linalg_eigh_out(const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
    TORCH_CHECK(false, "linalg_eigh.eigvals is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_eigvalsh(const at::Tensor & self, c10::string_view UPLO) {
    TORCH_CHECK(false, "linalg_eigvalsh is unsupported!");
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_eigvalsh_out(const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
    TORCH_CHECK(false, "linalg_eigvalsh.out is unsupported!");
}

} // anonymous namespace

TORCH_LIBRARY_IMPL(aten, XLA, m) {
  m.impl("_conj",
  TORCH_FN(wrapper___conj));
  m.impl("conj",
  TORCH_FN(wrapper__conj));
  m.impl("_conj_physical",
  TORCH_FN(wrapper___conj_physical));
  m.impl("conj_physical",
  TORCH_FN(wrapper__conj_physical));
  m.impl("conj_physical.out",
  TORCH_FN(wrapper_out_conj_physical_out));
  m.impl("conj_physical_",
  TORCH_FN(wrapper__conj_physical_));
  m.impl("isin.Tensor_Tensor",
  TORCH_FN(wrapper_Tensor_Tensor_isin));
  m.impl("isin.Tensor_Tensor_out",
  TORCH_FN(wrapper_Tensor_Tensor_out_isin_out));
  m.impl("logdet",
  TORCH_FN(wrapper__logdet));
  m.impl("frexp.Tensor",
  TORCH_FN(wrapper_Tensor_frexp));
  m.impl("frexp.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_frexp_out));
  m.impl("bitwise_left_shift.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_left_shift));
  m.impl("bitwise_left_shift.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_bitwise_left_shift_out));
  m.impl("bitwise_left_shift_.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_left_shift_));
  m.impl("bitwise_left_shift.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_bitwise_left_shift));
  m.impl("bitwise_left_shift.Tensor_Scalar_out",
  TORCH_FN(wrapper_Tensor_Scalar_out_bitwise_left_shift_out));
  m.impl("bitwise_left_shift_.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_bitwise_left_shift_));
  m.impl("bitwise_left_shift.Scalar_Tensor",
  TORCH_FN(wrapper_Scalar_Tensor_bitwise_left_shift));
  m.impl("bitwise_right_shift.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_right_shift));
  m.impl("bitwise_right_shift.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_bitwise_right_shift_out));
  m.impl("bitwise_right_shift_.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_right_shift_));
  m.impl("bitwise_right_shift.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_bitwise_right_shift));
  m.impl("bitwise_right_shift.Tensor_Scalar_out",
  TORCH_FN(wrapper_Tensor_Scalar_out_bitwise_right_shift_out));
  m.impl("bitwise_right_shift_.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_bitwise_right_shift_));
  m.impl("bitwise_right_shift.Scalar_Tensor",
  TORCH_FN(wrapper_Scalar_Tensor_bitwise_right_shift));
  m.impl("linalg_solve_triangular",
  TORCH_FN(wrapper__linalg_solve_triangular));
  m.impl("linalg_solve_triangular.out",
  TORCH_FN(wrapper_out_linalg_solve_triangular_out));
  m.impl("cholesky",
  TORCH_FN(wrapper__cholesky));
  m.impl("cholesky.out",
  TORCH_FN(wrapper_out_cholesky_out));
  m.impl("geqrf",
  TORCH_FN(wrapper__geqrf));
  m.impl("geqrf.a",
  TORCH_FN(wrapper_a_geqrf_out));
  m.impl("lu_solve",
  TORCH_FN(wrapper__lu_solve));
  m.impl("lu_solve.out",
  TORCH_FN(wrapper_out_lu_solve_out));
  m.impl("lu_unpack",
  TORCH_FN(wrapper__lu_unpack));
  m.impl("lu_unpack.out",
  TORCH_FN(wrapper_out_lu_unpack_out));
  m.impl("special_entr",
  TORCH_FN(wrapper__special_entr));
  m.impl("special_entr.out",
  TORCH_FN(wrapper_out_special_entr_out));
  m.impl("special_erfcx",
  TORCH_FN(wrapper__special_erfcx));
  m.impl("special_erfcx.out",
  TORCH_FN(wrapper_out_special_erfcx_out));
  m.impl("special_zeta",
  TORCH_FN(wrapper__special_zeta));
  m.impl("special_zeta.out",
  TORCH_FN(wrapper_out_special_zeta_out));
  m.impl("special_zeta.self_scalar",
  TORCH_FN(wrapper_self_scalar_special_zeta));
  m.impl("special_zeta.self_scalar_out",
  TORCH_FN(wrapper_self_scalar_out_special_zeta_out));
  m.impl("special_zeta.other_scalar",
  TORCH_FN(wrapper_other_scalar_special_zeta));
  m.impl("special_zeta.other_scalar_out",
  TORCH_FN(wrapper_other_scalar_out_special_zeta_out));
  m.impl("linalg_cholesky_ex",
  TORCH_FN(wrapper__linalg_cholesky_ex));
  m.impl("linalg_cholesky_ex.L",
  TORCH_FN(wrapper_L_linalg_cholesky_ex_out));
  m.impl("linalg_lu_factor_ex",
  TORCH_FN(wrapper__linalg_lu_factor_ex));
  m.impl("linalg_lu_factor_ex.out",
  TORCH_FN(wrapper_out_linalg_lu_factor_ex_out));
  m.impl("linalg_det",
  TORCH_FN(wrapper__linalg_det));
  m.impl("linalg_det.out",
  TORCH_FN(wrapper_out_linalg_det_out));
  m.impl("_det_lu_based_helper",
  TORCH_FN(wrapper___det_lu_based_helper));
  m.impl("linalg_lstsq",
  TORCH_FN(wrapper__linalg_lstsq));
  m.impl("linalg_lstsq.out",
  TORCH_FN(wrapper_out_linalg_lstsq_out));
  m.impl("linalg_eig",
  TORCH_FN(wrapper__linalg_eig));
  m.impl("linalg_eig.out",
  TORCH_FN(wrapper_out_linalg_eig_out));
  m.impl("linalg_eigh",
  TORCH_FN(wrapper__linalg_eigh));
  m.impl("linalg_eigh.eigvals",
  TORCH_FN(wrapper_eigvals_linalg_eigh_out));
  m.impl("linalg_eigvalsh",
  TORCH_FN(wrapper__linalg_eigvalsh));
  m.impl("linalg_eigvalsh.out",
  TORCH_FN(wrapper_out_linalg_eigvalsh_out));
}

} // anonymous namespace

namespace npu {


at::Tensor _conj(const at::Tensor & self) {
return wrapper___conj(self);
}

at::Tensor conj(const at::Tensor & self) {
return wrapper__conj(self);
}

at::Tensor _conj_physical(const at::Tensor & self) {
return wrapper___conj_physical(self);
}

at::Tensor conj_physical(const at::Tensor & self) {
return wrapper__conj_physical(self);
}

at::Tensor & conj_physical_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_conj_physical_out(self, out);
}

at::Tensor & conj_physical_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_conj_physical_out(self, out);
}

at::Tensor & conj_physical_(at::Tensor & self) {
return wrapper__conj_physical_(self);
}

at::Tensor isin(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
return wrapper_Tensor_Tensor_isin(elements, test_elements, assume_unique, invert);
}

at::Tensor & isin_out(at::Tensor & out, const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
return wrapper_Tensor_Tensor_out_isin_out(elements, test_elements, assume_unique, invert, out);
}

at::Tensor & isin_outf(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
return wrapper_Tensor_Tensor_out_isin_out(elements, test_elements, assume_unique, invert, out);
}

at::Tensor logdet(const at::Tensor & self) {
return wrapper__logdet(self);
}

::std::tuple<at::Tensor,at::Tensor> frexp(const at::Tensor & self) {
return wrapper_Tensor_frexp(self);
}

::std::tuple<at::Tensor &,at::Tensor &> frexp_out(at::Tensor & mantissa, at::Tensor & exponent, const at::Tensor & self) {
return wrapper_Tensor_out_frexp_out(self, mantissa, exponent);
}

::std::tuple<at::Tensor &,at::Tensor &> frexp_outf(const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
return wrapper_Tensor_out_frexp_out(self, mantissa, exponent);
}

at::Tensor bitwise_left_shift(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_bitwise_left_shift(self, other);
}

at::Tensor & bitwise_left_shift_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_bitwise_left_shift_out(self, other, out);
}

at::Tensor & bitwise_left_shift_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_bitwise_left_shift_out(self, other, out);
}

at::Tensor & bitwise_left_shift_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_bitwise_left_shift_(self, other);
}

at::Tensor bitwise_left_shift(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Tensor_Scalar_bitwise_left_shift(self, other);
}

at::Tensor & bitwise_left_shift_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Tensor_Scalar_out_bitwise_left_shift_out(self, other, out);
}

at::Tensor & bitwise_left_shift_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Tensor_Scalar_out_bitwise_left_shift_out(self, other, out);
}

at::Tensor & bitwise_left_shift_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Tensor_Scalar_bitwise_left_shift_(self, other);
}

at::Tensor bitwise_left_shift(const at::Scalar & self, const at::Tensor & other) {
return wrapper_Scalar_Tensor_bitwise_left_shift(self, other);
}

at::Tensor bitwise_right_shift(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_bitwise_right_shift(self, other);
}

at::Tensor & bitwise_right_shift_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_bitwise_right_shift_out(self, other, out);
}

at::Tensor & bitwise_right_shift_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_bitwise_right_shift_out(self, other, out);
}

at::Tensor & bitwise_right_shift_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_bitwise_right_shift_(self, other);
}

at::Tensor bitwise_right_shift(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Tensor_Scalar_bitwise_right_shift(self, other);
}

at::Tensor & bitwise_right_shift_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Tensor_Scalar_out_bitwise_right_shift_out(self, other, out);
}

at::Tensor & bitwise_right_shift_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Tensor_Scalar_out_bitwise_right_shift_out(self, other, out);
}

at::Tensor & bitwise_right_shift_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Tensor_Scalar_bitwise_right_shift_(self, other);
}

at::Tensor bitwise_right_shift(const at::Scalar & self, const at::Tensor & other) {
return wrapper_Scalar_Tensor_bitwise_right_shift(self, other);
}

at::Tensor linalg_solve_triangular(const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular) {
return wrapper__linalg_solve_triangular(self, B, upper, left, unitriangular);
}

at::Tensor & linalg_solve_triangular_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular) {
return wrapper_out_linalg_solve_triangular_out(self, B, upper, left, unitriangular, out);
}

at::Tensor & linalg_solve_triangular_outf(const at::Tensor & self, const at::Tensor & B, bool upper, bool left, bool unitriangular, at::Tensor & out) {
return wrapper_out_linalg_solve_triangular_out(self, B, upper, left, unitriangular, out);
}

at::Tensor cholesky(const at::Tensor & self, bool upper) {
return wrapper__cholesky(self, upper);
}

at::Tensor & cholesky_out(at::Tensor & out, const at::Tensor & self, bool upper) {
return wrapper_out_cholesky_out(self, upper, out);
}

at::Tensor & cholesky_outf(const at::Tensor & self, bool upper, at::Tensor & out) {
return wrapper_out_cholesky_out(self, upper, out);
}

::std::tuple<at::Tensor,at::Tensor> geqrf(const at::Tensor & self) {
return wrapper__geqrf(self);
}

::std::tuple<at::Tensor &,at::Tensor &> geqrf_out(at::Tensor & a, at::Tensor & tau, const at::Tensor & self) {
return wrapper_a_geqrf_out(self, a, tau);
}

::std::tuple<at::Tensor &,at::Tensor &> geqrf_outf(const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
return wrapper_a_geqrf_out(self, a, tau);
}

at::Tensor lu_solve(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
return wrapper__lu_solve(self, LU_data, LU_pivots);
}

at::Tensor & lu_solve_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
return wrapper_out_lu_solve_out(self, LU_data, LU_pivots, out);
}

at::Tensor & lu_solve_outf(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
return wrapper_out_lu_solve_out(self, LU_data, LU_pivots, out);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> lu_unpack(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
return wrapper__lu_unpack(LU_data, LU_pivots, unpack_data, unpack_pivots);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> lu_unpack_out(at::Tensor & P, at::Tensor & L, at::Tensor & U, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
return wrapper_out_lu_unpack_out(LU_data, LU_pivots, unpack_data, unpack_pivots, P, L, U);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> lu_unpack_outf(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
return wrapper_out_lu_unpack_out(LU_data, LU_pivots, unpack_data, unpack_pivots, P, L, U);
}

at::Tensor special_entr(const at::Tensor & self) {
return wrapper__special_entr(self);
}

at::Tensor & special_entr_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_entr_out(self, out);
}

at::Tensor & special_entr_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_entr_out(self, out);
}

at::Tensor special_erfcx(const at::Tensor & self) {
return wrapper__special_erfcx(self);
}

at::Tensor & special_erfcx_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_erfcx_out(self, out);
}

at::Tensor & special_erfcx_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_erfcx_out(self, out);
}

at::Tensor special_zeta(const at::Tensor & self, const at::Tensor & other) {
return wrapper__special_zeta(self, other);
}

at::Tensor & special_zeta_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_special_zeta_out(self, other, out);
}

at::Tensor & special_zeta_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_special_zeta_out(self, other, out);
}

at::Tensor special_zeta(const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_special_zeta(self, other);
}

at::Tensor & special_zeta_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_out_special_zeta_out(self, other, out);
}

at::Tensor & special_zeta_outf(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_self_scalar_out_special_zeta_out(self, other, out);
}

at::Tensor special_zeta(const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_special_zeta(self, other);
}

at::Tensor & special_zeta_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_out_special_zeta_out(self, other, out);
}

at::Tensor & special_zeta_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_other_scalar_out_special_zeta_out(self, other, out);
}

::std::tuple<at::Tensor,at::Tensor> linalg_cholesky_ex(const at::Tensor & self, bool upper, bool check_errors) {
return wrapper__linalg_cholesky_ex(self, upper, check_errors);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_out(at::Tensor & L, at::Tensor & info, const at::Tensor & self, bool upper, bool check_errors) {
return wrapper_L_linalg_cholesky_ex_out(self, upper, check_errors, L, info);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_outf(const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
return wrapper_L_linalg_cholesky_ex_out(self, upper, check_errors, L, info);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_lu_factor_ex(const at::Tensor & A, bool pivot, bool check_errors) {
return wrapper__linalg_lu_factor_ex(A, pivot, check_errors);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_factor_ex_out(at::Tensor & LU, at::Tensor & pivots, at::Tensor & info, const at::Tensor & A, bool pivot, bool check_errors) {
return wrapper_out_linalg_lu_factor_ex_out(A, pivot, check_errors, LU, pivots, info);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_lu_factor_ex_outf(const at::Tensor & A, bool pivot, bool check_errors, at::Tensor & LU, at::Tensor & pivots, at::Tensor & info) {
return wrapper_out_linalg_lu_factor_ex_out(A, pivot, check_errors, LU, pivots, info);
}

at::Tensor linalg_det(const at::Tensor & self) {
return wrapper__linalg_det(self);
}

at::Tensor & linalg_det_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_linalg_det_out(self, out);
}

at::Tensor & linalg_det_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_linalg_det_out(self, out);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> _det_lu_based_helper(const at::Tensor & self) {
return wrapper___det_lu_based_helper(self);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> linalg_lstsq(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
return wrapper__linalg_lstsq(self, b, rcond, driver);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> linalg_lstsq_out(at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
return wrapper_out_linalg_lstsq_out(self, b, rcond, driver, solution, residuals, rank, singular_values);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> linalg_lstsq_outf(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
return wrapper_out_linalg_lstsq_out(self, b, rcond, driver, solution, residuals, rank, singular_values);
}

::std::tuple<at::Tensor,at::Tensor> linalg_eig(const at::Tensor & self) {
return wrapper__linalg_eig(self);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_eig_out(at::Tensor & eigenvalues, at::Tensor & eigenvectors, const at::Tensor & self) {
return wrapper_out_linalg_eig_out(self, eigenvalues, eigenvectors);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_eig_outf(const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
return wrapper_out_linalg_eig_out(self, eigenvalues, eigenvectors);
}

::std::tuple<at::Tensor,at::Tensor> linalg_eigh(const at::Tensor & self, c10::string_view UPLO) {
return wrapper__linalg_eigh(self, UPLO);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_out(at::Tensor & eigvals, at::Tensor & eigvecs, const at::Tensor & self, c10::string_view UPLO) {
return wrapper_eigvals_linalg_eigh_out(self, UPLO, eigvals, eigvecs);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_outf(const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
return wrapper_eigvals_linalg_eigh_out(self, UPLO, eigvals, eigvecs);
}

at::Tensor linalg_eigvalsh(const at::Tensor & self, c10::string_view UPLO) {
return wrapper__linalg_eigvalsh(self, UPLO);
}

at::Tensor & linalg_eigvalsh_out(at::Tensor & out, const at::Tensor & self, c10::string_view UPLO) {
return wrapper_out_linalg_eigvalsh_out(self, UPLO, out);
}

at::Tensor & linalg_eigvalsh_outf(const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
return wrapper_out_linalg_eigvalsh_out(self, UPLO, out);
}

} // namespace npu

} // namespace at
